# -*- coding: utf-8 -*-
"""titanic-logistic-regression-and-xgbclassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sXvakLbICUFtOtL1JtoGx5ZjWTOKZ0ro

**An exercise to demonstrate how to use Python to perform various tasks, including EDA, cleaning, visualizations, regressive analysis, confusion matrix, and XGBClassifier construction, fitting and evaluating.**

Data Source: https://www.kaggle.com/c/titanic,

# TASK #1: IMPORT LIBRARIES AND DATASETS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Read the data using pandas dataframe
test_df = pd.read_csv('/kaggle/input/titanic/test.csv')
train_df = pd.read_csv('/kaggle/input/titanic/train.csv')
print(test_df.shape)
print(train_df.shape)
#test data has one less row, 'Survived', because these datasets are meant for the Titanic Competition.
#The test data is missing that row because it is the value you are trying to predict.

# Show the first few lines of data

train_df.head()

"""## **PERFORM DATA EDA and Visualization**"""

# Let's count the number of survivors and non-survivors in the train data.
print(train_df['Survived'].value_counts())

survived_df=train_df[train_df['Survived'] == 1]
died_df=train_df[train_df['Survived'] == 0]

# Count the number survived and those that did not survive in the train data.
print("Total =", len(train_df))

print("Number of passengers who survived =", len(survived_df))
print("Percentage Survived =", round(1.0 * len(survived_df) / len(train_df) * 100.0, 2), "%")

print("Number of passengers who did not Survive =", len(died_df))
print("Percentage who did not survive =", round(1. * len(died_df) / len(train_df) * 100.0, 2), "%")

# Bar Chart to indicate the number of people who survived based on their class.
# Those who travelled first class had a higher chance of survival.
plt.figure(figsize=(5, 3))
sns.countplot(x='Pclass', hue='Survived', data=train_df)
plt.show()

# Bar Chart to indicate the number of people survived based on their Parch status
#(those there with parents or children)
plt.figure(figsize = (5, 3))

sns.countplot(x = 'Parch', hue = 'Survived', data = train_df)

plt.show()

# Bar Chart to indicate the number of people survived based on their gender.
# Females had a higher chance of survival.
plt.figure(figsize = [5, 3])
plt.subplot(211)
sns.countplot(x = 'Sex', data = train_df)
plt.subplot(212)
sns.countplot(x = 'Sex', hue = 'Survived', data = train_df)

# This shows how those without siblings on board were twice as likely to die, and roughly 25% more likely
#to live if there was one sibling.  However, 2 or more siblings didn't make much of a difference.

plt.figure(figsize = [10, 6])

sns.countplot(x = 'SibSp', hue = 'Survived', data = train_df)

#A histogram from Seaborn to compare survival based on fare.  Travellers who didn't pay a fare (staff) were
#much more likely to die.
# Bar Chart to indicate the number of people who survived based on their class.
# Those who travelled first class had a higher chance of survival.

#warning supression for tidy plot
import warnings

warnings.filterwarnings(
    'ignore',
    message=".*use_inf_as_na option is deprecated and will be removed in a future version.*",
    category=FutureWarning
)
sns.histplot(data=died_df, x='Fare', bins=10, binwidth=20, label="Didn't Survive", multiple='dodge')
sns.histplot(data=survived_df, x='Fare', bins=10, binwidth=20, label="Survived", multiple='dodge')
plt.title("Survived vs Didn't Survive, Based on Fare")
plt.legend()
plt.show()

"""**PERFORM DATA CLEANING AND FEATURE ENGINEERING**"""

# Let's explore which columns are missing data
plt.figure(figsize=(3,2))
sns.heatmap(train_df.isnull(), yticklabels = False, cbar = False, cmap="Blues")

train_df

# Drop the cabin column and unnecessary columns and test with inplace = true and false
train_df = train_df.drop(['Cabin','Name', 'Ticket','Embarked'], axis =1)

train_df.head()

#Repeat for test data
test_df.drop(['Cabin','Name', 'Ticket','Embarked'], axis =1, inplace = True)

# We need to deal with the missing values for age.
# Let's get the average age for male (~29) and female (~25)
plt.figure(figsize=(4, 2))
sns.boxplot(x = 'Sex', y = 'Age', data = train_df)

age_fill = train_df.groupby(['Pclass'])['Age'].median()

def fill_age(row):
    if pd.isnull(row['Age']):
        median_age = age_fill.get((row['Pclass']), None)
        if median_age is None:
            # Fallback to median of just Pclass
            median_age = age_fill.get(('Missing', row['Pclass']), np.nan)
        if pd.isnull(median_age):
            # Fallback to overall median of Age if still NaN
            median_age = train_df['Age'].median()
        return median_age
    else:
        return row['Age']

train_df['Age'] = train_df.apply(fill_age, axis=1)
test_df['Age'] = test_df.apply(fill_age, axis=1)

print(train_df.isna().sum())
print(test_df.isna().sum())

#fill in na value
test_df['Fare'] = test_df['Fare'].fillna(1)
test_df.head()

print(test_df.isna().sum())

# Let's view the data one more time!
plt.figure(figsize=(4,2))
sns.heatmap(test_df.isnull(), yticklabels = False, cbar = False, cmap="Blues")
#Success!  The heatmap is empty! (for both train_df and test_df)

# One column is needed to represent male or female.  First, use pd.get_dummies to assign a 1 or 0, then
#drop #Sex_male
import pandas as pd
train_df = pd.get_dummies(train_df, columns=['Sex'], dtype=int)

train_df = train_df.drop('Sex_male', axis=1)

train_df

#repeat for test
test_df = pd.get_dummies(test_df, columns=['Sex'], dtype=int)

#remove extra row, sex male.
test_df = test_df.drop('Sex_male', axis =1)
test_df

print(train_df.shape)
print(test_df.shape)

"""# TRAIN LOGISTIC REGRESSION CLASSIFIER MODEL"""

#define train variables
#Define X predictor variables
x = train_df.copy()
y = train_df['Survived']
x = train_df.drop('Survived', axis=1)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_train, X_val, y_train, y_val = train_test_split(x,y, test_size=0.3, random_state=42)

print(X_train.shape)
print(X_val.shape)
print(y_train.shape)
print(y_val.shape)

"""### MODEL BUILDING"""

#Build a logistic regression model and fit it to the data
clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)

#Obtain parameter estimates
clf.coef_

clf.intercept_

# save predictions
y_preds = clf.predict(X_val)

clf.predict(X_val)

#print out the predicted probabilities on each data point
clf.predict_proba(X_val)[::,-1]

"""### ASSESS TRAINED MODEL PERFORMANCE"""

#Import:
from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

#trained model performance on its val(validation) data.
print('Accuracy', '%.3f' % accuracy_score(y_val, y_preds))
print('Precision:', '%.3f' % precision_score(y_val, y_preds))
print('Recall:', '%.3f' % recall_score(y_val, y_preds))
print('F1 Score:', '%.3f' % f1_score(y_val, y_preds))

"""In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
"""

#Create a confusion matrix for the training data.
import sklearn.metrics as metrics
cm = metrics.confusion_matrix(y_val, y_preds, labels=clf.classes_)
disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_,
                                 )
disp.plot()

#X, test_df = X.align(test_df, join='inner', axis=0, fill_value=None)

import xgboost as xgb
from xgboost import XGBClassifier
xgb = XGBClassifier(enable_categorical=True, tree_method='hist', use_label_encoder=False, random_state=0)

#set param for XGboost
# Set parameters for XGBoost
cv_params = {'max_depth': [3],
    'learning_rate': [0.1],
    'n_estimators': [100],
}

#define criteria as scoring
scoring = {'accuracy', 'precision', 'recall', 'f1'}

#construct GridSearch
from sklearn.model_selection import GridSearchCV, train_test_split
xgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='f1')

#fit GridSearch model to training data
xgb_cv.fit(X_train, y_train)

#apply model to predict on real test data
y_pred = xgb_cv.predict(test_df)

y_pred.shape

# Assuming `test_cleaned` is your test data and you've made predictions
# Replace 'model.predict()' with the actual method you're using to get predictions
predictions = y_pred

# Create a DataFrame with the required columns
submission = pd.DataFrame({
    'PassengerId': test_df['PassengerId'],  # Use the original test data's PassengerId
    'Survived': predictions               # Your model's predictions
})

# Save the submission DataFrame to a CSV file
submission.to_csv('submission_titanic.csv', index=False)

print("Your submission was successfully saved")